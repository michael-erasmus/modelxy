{
  
    
        "post0": {
            "title": "Think Columns",
            "content": "This is a first installment in a series of posts I plan on writing related to the basics of analytical SQL databases. This content probably most useful to junior or aspriring analytics engineers. . What is a columnar database? . Most well known modern data warehouses (Snowflake, BigQuery, Redshift, etc) are column-oriented, or just columnar databases. This means that unlike more mainstream SQL databases, data is stored in columns instead of rows. So for any particular table in the database, all the data for a certain column will be stored sequentially. . Why does this matter? Well, this means that typical queries that you would need to do analysis could be much faster on huge datasets. . Take a very simple aggregation such as this: . select sum(amount) from transactions . Let&#39;s say you&#39;re running this on a really big database table with millions of records. In a columnar database, since all the data in the amount column is optimized to be stored together, this query will be much faster than on a traditional database, especially if you haven&#39;t optimized the table for doing aggregations on that field. . We often do a lot of these kinds of aggregations (MIN, MAX, SUM, COUNT, AVG, etc) for analytics work, which columnar databases can do really well. . Another benefit of being columnar is that the database can optimize well for performing as much work in memory as possible, and easily distribute workloads in parallel onto a cluster of computers. In our query above, the DB engine only needs to load up data for the column amount to perform the count, and since it&#39;s all stored together, this is easy to do without having to scan the full rows. . If a table has millions or even billions of records, some columnar databases can store segments of all the data in our column on different nodes in of cluster, which can individualy compute the sum of each segement seperately, which can then be tallied into a final sum fairly quickly. In this way you can scale an operation that would have been slow or even impossible on a single machine to a cluster of machines easily. . Why should care about if my database is columnar? . In a lot of ways, when your using a colummar database, the database engine doesn&#39;t need you to know much about the fact that it&#39;s columnar at all. On the surface, you can access it very much like you would a row-oriented database such as PostgreSQL. At a semantic level, most database entities and queries will appear almost identical. You&#39;ll can also stick to the familiar SQL database nomenclature, using terms such as tables, rows, columns and fields. . Columnar databases do, however, significantly change the way you think about modeling and querying data. Even though you don&#39;t have to be concerned about the low-level details (unless you&#39;re interested), having a high-level understanding of the important concepts will help guide and inform your decisions on how you ingest, store and query your data. . For instance, in a columnar database, it&#39;s fine to have a huge table with many columns that might be sparsely populated. As long as your not selecting a lot of columns at once, and only performing aggregations on certain columns, this will still be super performant. . Since tables can have many columns, you might not need to normalize data as much, which is also a great win for analytics. . What are columnar databases not good for? . The flip side of all of the benefits of a columnar database is that a simple select * query that might only return one or a few rows can be surprisingly slow. . Unlike traditional SQL databases, fields related to the same row in a table might not be stored close to each other, and could often be stored in different &#39;slices&#39; of your database, which could mean different machines in your cluster. So, in a query, selecting all of your table&#39;s columns for any particular row might force the database to find and put together a whole lot of scattered datapoints, which can result in a slower than expected execution time. . That doesn&#39;t mean that you can&#39;t write fast queries that need access to many or all of the columns in at all, it&#39;s still perfectly possible but it might mean that need spend more time thinking about how design and optimize your database. . This is why a columnar database won&#39;t make for a great database to power a normal web or client-server application, where you&#39;re often reading in all the columns for a small number of rows. Another reason why columnar databases aren&#39;t that great for these kinds of applications is that it&#39;s not really optimized for writing small batches of records to the database. . But it can perform analysis and number crunching on huge datasets incredibly quickly if you know what your doing! . Implementing the concept with code . Let&#39;s make the concept of a column vs row oriented database more concrete by writing some code. . We&#39;ll build a very, very basic &#39;database&#39; implementation in Python. . First we&#39;ll create a classic-style in-memory database table to store a list of transactions. Each &#39;row&#39; is a dictionary with values for each column. . transactions =[ {&quot;id&quot;: 1, &quot;customer_id&quot;: 1, &quot;product_id&quot;: 1, &quot;amount&quot;: 9.99}, {&quot;id&quot;: 2, &quot;customer_id&quot;: 1, &quot;product_id&quot;: 2, &quot;amount&quot;: 4.99}, {&quot;id&quot;: 3, &quot;customer_id&quot;: 2, &quot;product_id&quot;: 3, &quot;amount&quot;: 25.99}, ] . Now we can &#39;query&#39; our database. Let&#39;s say we want to calculate our total revenue, the sum of all the amount columns. This function will loop through each transaction, read the amount and then sum it all together. . def get_total_transaction_revenue(): &quot;&quot;&quot; Return the total revenue from all transactions &quot;&quot;&quot; return sum([t[&quot;amount&quot;] for t in transactions]) get_total_transaction_revenue() . 40.97 . Let&#39;s benchmark this function. We create a benchmark function that will run our query 1 million times and see how long it takes to run. . def benchmark(func): import timeit num_runs = 10**6 duration = timeit.Timer(func).timeit(number = num_runs) print(f&quot;{num_runs} runs took {duration} seconds&quot;) . And we run the benchmark: . benchmark(get_total_transaction_revenue) . 1000000 runs took 1.9082819640170783 seconds . Ok, so let&#39;s make a columnar version of our &#39;table&#39;. Instead of starting from scratch I&#39;ll convert our existing transactions data structure. . The main difference to note here is that our table is now a dictionary with keys for each column, which maps to a list of values for each row. . transactions_columnar = { &quot;id&quot;: [t[&quot;id&quot;] for t in transactions], &quot;customer_id&quot;: [t[&quot;customer_id&quot;] for t in transactions], &quot;product_id&quot;: [t[&quot;product_id&quot;] for t in transactions], &quot;amount&quot;: [t[&quot;amount&quot;] for t in transactions], } transactions_columnar . {&#39;id&#39;: [1, 2, 3], &#39;customer_id&#39;: [1, 1, 2], &#39;product_id&#39;: [1, 2, 3], &#39;amount&#39;: [9.99, 4.99, 25.99]} . Now we can build a similar &#39;query&#39; function to calculate our total revenue on our column-oriented table. Instead of having to loop through each row, we can just sum up the amount column directly . def get_total_transaction_revenue_cols(): &quot;&quot;&quot; Return the total revenue from all transactions &quot;&quot;&quot; return sum(transactions_columnar[&quot;amount&quot;]) get_total_transaction_revenue_cols() . 40.97 . This gives us the same result as we got before. . assert get_total_transaction_revenue() == get_total_transaction_revenue_cols() . Let&#39;s benchmark this function then: . benchmark(get_total_transaction_revenue_cols) . 1000000 runs took 1.1226091338321567 seconds . Not a massive difference, but we can show that this is a faster implementation. . Even on this toy example we can see the benefit of using a column-oriented datastructure for certain aggregation patterns. .",
            "url": "https://modelxy.com/analytics_engineering/sql/2022/08/12/think-columns.html",
            "relUrl": "/analytics_engineering/sql/2022/08/12/think-columns.html",
            "date": " • Aug 12, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Hello World!",
            "content": "Welcome to my new blog, modelxy! This has been a long time coming, the last time I&#39;ve blogged was around 2017 and I can&#39;t believe all that time has gone by without me sharing much to the world. . Well that stops today. I&#39;m resolved to get back into the swing of things. I might be a little rusty and need some practice, but I have a lot to say and a lot to share, so here goes! . One of the big reasons I&#39;ve been hesitant to start writing on a blog has been the friction around sharing code easily. I really like being able to understand and explain concepts through the lens of writing code. Using most blog engines makes for a sub-optimal solution of running code somewhere else and then copy-pasting it into formatted code snippets. I much rather prefer mixing code and prose like I can do in a Jupyter Notebook. . Thus this blog is powered by Fastpages, and as much as possible I&#39;ll be writing posts as notebooks. . In fact, this post was written as a Notebook! To prove it, here is an rendering of the Sierpiński triangle made using the Chaos Game Method1. . . Adapted from this example&#8617; . | import numpy as np import matplotlib.pyplot as plt plt.figure(figsize=(20, 10), dpi=80) image_size = np.array((1000, 1000)) r, nsides, npts = 0.5, 3, 500000 # The vertices of a regular polygon with nsides sides and a vertex pointing up, # calculated on the unit circle centered at (1,1) polygon = [(np.cos(phi)+1, np.sin(phi)+1) for phi in np.arange(0, 2*np.pi, 2.*np.pi/nsides)] # Map vertices to image pixels pts = (np.array(polygon) * 0.5 * (image_size-1)) # Initial point at centre of polygon p1 = np.array(image_size/2) aimg = np.zeros(image_size) # Play the Chaos Game! for i in range(npts): irow = np.random.randint(nsides) p2 = pts[irow] p1 = (p1 * r + p2 * (1-r)) aimg[tuple(p1.astype(int))] += 1 plt.pcolor(aimg) plt.show() . Isn&#39;t that quite beautiful? When I first learned about fractals I was deeply struck by how such complex structure can emerge from very simple building blocks. When using a process such as the chaos game to construct our fractal, it&#39;s incredible that order can emerge from chaos. . These are the lessons I take when it comes to my humble new blog. My hope is that over time some each nugget of information I can share can build up to a greater whole. I plan to write about a diverse set of topics, whatever I find to be intellectually interesting. My wish is that some kind of order can emerge from this chaos. . The world is full of beautiful ideas and this blog is a celebration of all that beauty. Let&#39;s have fun exploring together! .",
            "url": "https://modelxy.com/general/2022/08/03/hello-world.html",
            "relUrl": "/general/2022/08/03/hello-world.html",
            "date": " • Aug 3, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Michael Erasmus. My pronouns are he/him. . I currently work for DonorsChoose as the Director of Data Science and Analytics. DonorsChoose is an amazing non-profit org with the mission to make it easy for anyone to help a teacher in need, moving us closer to a nation where students in every community have the tools and experiences they need for a great education. . Before DonorsChoose, I worked at Buffer. . The list of things that make me happy . Work | Reading and learning for the sake of it | Writing | Traveling | Surfing and swimming | Cooking | Playing Ukulele | My Family | . I write about adventures in Data Science &amp; Engineering, Machine Learning, SQL, Python and other things! . I live in beautiful Santa Cruz, CA . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://modelxy.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://modelxy.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}